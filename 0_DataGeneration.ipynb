{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17d254f9-b908-4b6b-87cb-f30f1dab1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext ,Row\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sequence, col, expr, rand, when, lit, concat\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb5476b-c3fc-4c8a-8f92-30c443e7bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/11 16:30:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/11 16:30:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                .appName(\"spark_session\") \\\n",
    "                .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "                .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "                .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "                .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "                .config(\"spark.sql.htl.check\", \"false\") \\\n",
    "                .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonmptyLocation\", \"true\") \\\n",
    "                .enableHiveSupport() \\\n",
    "                .master(\"local[*]\") \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed7231-56df-4b08-aaf2-14279b3824d6",
   "metadata": {},
   "source": [
    "#### Load config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e484920e-84ec-4744-8f3b-bcb48d16608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------------+--------------+---------------+-------------+----------+--------------------+\n",
      "|time_series_prep_sdate|time_series_prep_edate|train_start_date|train_end_date|test_start_date|test_end_date|score_date|model_version_prefix|\n",
      "+----------------------+----------------------+----------------+--------------+---------------+-------------+----------+--------------------+\n",
      "|            2021-04-01|            2023-09-14|      2022-06-01|    2023-06-01|     2023-07-01|   2023-09-01|2023-10-01|            20230717|\n",
      "+----------------------+----------------------+----------------+--------------+---------------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load config\n",
    "# ==========================================\n",
    "\n",
    "#config = spark.table(\"db.xs0_config\")\n",
    "\n",
    "config = spark.read.parquet('xs0_config.parquet')\n",
    "\n",
    "time_series_prep_sdate = config.collect()[0][0]\n",
    "time_series_prep_sdate = config.collect()[0][0]\n",
    "time_series_prep_edate = config.collect()[0][1]\n",
    "train_start_date = config.collect()[0][2]\n",
    "train_end_date = config.collect()[0][3]\n",
    "test_start_date = config.collect()[0][4]\n",
    "test_end_date = config.collect()[0][5]\n",
    "score_date = config.collect()[0][6]\n",
    "model_version_prefix = config.collect()[0][7]\n",
    "\n",
    "config.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde682b-66f2-40d9-8439-458f4684f2df",
   "metadata": {},
   "source": [
    "## Account Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb4f235a-2cfe-4ec8-ba1d-e8272bcbfb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the number of observations\n",
    "num_observations = 1000000\n",
    "\n",
    "# Generate random dates within a specific range\n",
    "def random_date(start_date, end_date):\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    random_date = start_date + timedelta(days=random_number_of_days)\n",
    "    return random_date\n",
    "\n",
    "# Generate random customer numbers and transaction amounts\n",
    "customer_nos = [random.randint(1000, 9999) for _ in range(num_observations)]\n",
    "trn_dates = [random_date(datetime(2020, 1, 1), datetime(2023, 1, 1)) for _ in range(num_observations)]\n",
    "lcy_amounts = [random.uniform(0, 1000) for _ in range(num_observations)]\n",
    "drcr_inds = ['D' if random.random() < 0.5 else 'C' for _ in range(num_observations)]\n",
    "ac_branches = [random.randint(1, 10) for _ in range(num_observations)]\n",
    "financial_cycles = [random.randint(1, 12) for _ in range(num_observations)]\n",
    "period_codes = [random.randint(1, 5) for _ in range(num_observations)]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"trn_dt\": trn_dates,\n",
    "    \"lcy_amount\": lcy_amounts,\n",
    "    \"drcr_ind\": drcr_inds,\n",
    "    \"ac_branch\": ac_branches,\n",
    "    \"financial_cycle\": financial_cycles,\n",
    "    \"period_code\": period_codes\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"customer_no\"] = customer_nos\n",
    "df[\"sign_D\"] = (df[\"drcr_ind\"] == \"D\").astype(int)\n",
    "df[\"sign_C\"] = 1 - df[\"sign_D\"]\n",
    "df[\"lcy_dr\"] = df[\"lcy_amount\"] * df[\"sign_D\"]\n",
    "df[\"lcy_cr\"] = df[\"lcy_amount\"] * df[\"sign_C\"]\n",
    "\n",
    "# Extract year and month from trn_dt\n",
    "df[\"year\"] = df[\"trn_dt\"].dt.year\n",
    "df[\"month\"] = df[\"trn_dt\"].dt.month\n",
    "\n",
    "# Save the synthetic dataset as a Parquet file\n",
    "df.to_parquet(\"df_ubs_sttm_cust_account.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4156538-afca-4494-a420-f73dc39f7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/09/11 18:00:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "#==================\n",
    "#\n",
    "# 0. Columns for selection, years, months\n",
    "#\n",
    "#=================\n",
    "\n",
    "columns_for_selection = [\"cust_no\", \"trn_dt\", \"lcy_amount\", \"sign_D\", \"ac_branch\", \"financial_cycle\", \"period_code\"]\n",
    "# years = ['2018', '2019', '2020', '2021', '2022']\n",
    "start_year = 2021\n",
    "end_year = 2023\n",
    "start_month = 1\n",
    "end_month = 12\n",
    "\n",
    "# months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "#==================\n",
    "#\n",
    "# 1. Read the table and apply filters\n",
    "#\n",
    "#=================\n",
    "\n",
    "table = spark.read.parquet(\"df_ubs_sttm_cust_account.parquet\").select(\n",
    "    \"trn_dt\",\n",
    "    \"lcy_amount\",\n",
    "    \"drcr_ind\",\n",
    "    \"customer_no\",\n",
    "    F.col(\"year\").alias(\"trn_year\"),\n",
    "    F.col(\"month\").alias(\"trn_month\")\n",
    ").filter((F.col('trn_year') >= start_year) & (F.col('trn_year') <= end_year) & (F.col('trn_month') >= start_month) & (F.col('trn_month') <= end_month))\n",
    "\n",
    "table = table\\\n",
    "        .withColumn('sign_D', F.when(col('drcr_ind')=='D', 1).otherwise(0))\\\n",
    "        .withColumn('sign_C', (1-F.col(\"sign_D\")))\\\n",
    "        .withColumn(\"lcy_dr\", F.col(\"lcy_amount\") * F.col(\"sign_D\"))\\\n",
    "        .withColumn(\"lcy_cr\", F.col(\"lcy_amount\") * F.col(\"sign_C\"))\n",
    "\n",
    "#==================\n",
    "#\n",
    "# 2. Apply groupBy operation to get customer year-month\n",
    "#\n",
    "#=================\n",
    "\n",
    "cols_for_final_table = [\"lcy_dr\",\"lcy_cr\",\"sign_D\",\"sign_C\",\"customer_no\",\"trn_year\",\"trn_month\"]\n",
    "\n",
    "table_grouped = table.select(cols_for_final_table).groupBy([\"customer_no\", \"trn_year\", \"trn_month\"]).agg(\n",
    "    F.sum(\"lcy_dr\").alias(\"trn_sum_D\"),\n",
    "    F.sum(\"lcy_cr\").alias(\"trn_sum_c\"),\n",
    "    F.sum(\"sign_D\").alias(\"trn_count_D\"),\n",
    "    F.sum(\"sign_C\").alias(\"trn_count_C\")\n",
    ").write.mode('overwrite').parquet(\"xs2_account_transactions_grouped_\"+model_version_prefix+\".parquet\")\n",
    "\n",
    "\n",
    "# Step 2 - create complete tseries\n",
    "\n",
    "# Convert string dates to date objects\n",
    "sdate = parse(time_series_prep_sdate).date()\n",
    "edate = parse(time_series_prep_edate).date()\n",
    "\n",
    "dates = []\n",
    "while sdate <= edate:\n",
    "    dates.append(sdate)\n",
    "    sdate = sdate + relativedelta(months=1)\n",
    "\n",
    "dates_df = spark.createDataFrame([(d,) for d in dates], ['__PERIOD'])\n",
    "\n",
    "df_transactions = spark.read.parquet(\"xs2_account_transactions_grouped_\"+model_version_prefix+\".parquet\")\n",
    "\n",
    "# # NEED TO CREATE A __PERIOD FIELD FROM trn_year and trn_month in the format yyyy-mm-dd where dd is always 01\n",
    "df_transactions = df_transactions.withColumn(\"__PERIOD\", F.to_timestamp(F.concat(F.col(\"trn_year\"), F.lit(\"-\"), F.col(\"trn_month\"), F.lit(\"-01\"))))\n",
    "df_tseries = df_transactions.select(\"customer_no\").distinct().crossJoin(dates_df).orderBy([\"customer_no\",\"__PERIOD\"])\n",
    "df_tseries = df_tseries.join(df_transactions, [\"customer_no\",\"__PERIOD\"], \"left\").fillna(0)\n",
    "df_tseries.write.mode('overwrite').parquet(\"xs2_account_transactions_tseries_\"+model_version_prefix+\".parquet\")\n",
    "\n",
    "# Step 3 - calculate moving averages\n",
    "\n",
    "# 6) Calculate MAs\n",
    "df_tseries = spark.read.parquet(\"xs2_account_transactions_tseries_\"+model_version_prefix+\".parquet\")\n",
    "\n",
    "# Get MAs for each column\n",
    "value_cols = [\n",
    "    \"DR_total\",\n",
    "    \"DR_min\",\n",
    "    \"DR_max\",\n",
    "    \"DR_count\",\n",
    "    \"CR_total\",\n",
    "    \"CR_min\",\n",
    "    \"CR_max\",\n",
    "    \"CR_count\",\n",
    "    \"TR_count\"\n",
    "]\n",
    "\n",
    "      \n",
    "            \n",
    "w1 = Window()\\\n",
    "    .partitionBy(F.col(\"customer_no\"))\\\n",
    "    .orderBy(F.col(\"__PERIOD\"))\\\n",
    "    .rowsBetween(-2, 0)   \n",
    "    \n",
    "w2 = Window()\\\n",
    "    .partitionBy(F.col(\"customer_no\"))\\\n",
    "    .orderBy(F.col(\"__PERIOD\"))\\\n",
    "    .rowsBetween(-5, -3)\n",
    "            \n",
    "w3 = Window()\\\n",
    "    .partitionBy(F.col(\"customer_no\"))\\\n",
    "    .orderBy(F.col(\"__PERIOD\"))\\\n",
    "    .rowsBetween(-11, -6)\n",
    "            \n",
    "final_table = df_tseries.withColumn(\"tfeat_DR_total_1_to_3m\",  F.avg(\"trn_sum_D\").over(w1))\\\n",
    "                       .withColumn(\"tfeat_DR_total_7_to_12m\",  F.avg(\"trn_sum_D\").over(w3))\\\n",
    "                       .withColumn(\"tfeat_DR_SPIKE\",  F.col(\"trn_sum_D\")/F.col(\"tfeat_DR_total_1_to_3m\"))\\\n",
    "                       .withColumn(\"tfeat_DR_TREND\",  F.col(\"tfeat_DR_total_1_to_3m\")/F.col(\"tfeat_DR_total_7_to_12m\"))\\\n",
    "                       .withColumn(\"tfeat_CR_total_1_to_3m\",  F.avg(\"trn_sum_c\").over(w1))\\\n",
    "                       .withColumn(\"tfeat_CR_total_7_to_12m\",  F.avg(\"trn_sum_c\").over(w3))\\\n",
    "                       .withColumn(\"tfeat_CR_SPIKE\",  F.col(\"trn_sum_c\")/F.col(\"tfeat_CR_total_1_to_3m\"))\\\n",
    "                       .withColumn(\"tfeat_CR_TREND\",  F.col(\"tfeat_CR_total_1_to_3m\")/F.col(\"tfeat_CR_total_7_to_12m\"))\\\n",
    "                       .withColumn(\"tfeat_DRCount_average_1_to_3m\", F.avg(\"trn_count_D\").over(w1))\\\n",
    "                       .withColumn(\"tfeat_DRCount_average_7_to_12m\", F.avg(\"trn_count_D\").over(w3))\\\n",
    "                       .withColumn(\"tfeat_CRCount_average_1_to_3m\", F.avg(\"trn_count_C\").over(w1))\\\n",
    "                       .withColumn(\"tfeat_CRCount_average_7_to_12m\", F.avg(\"trn_count_C\").over(w3))\n",
    "\n",
    "\n",
    "final_table = final_table.withColumn(\"__PERIOD\", F.add_months(\"__PERIOD\",1))\n",
    "\n",
    "# save the final table\n",
    "final_table.write.mode('overwrite').parquet(\"xs2_account_transactions_final_\"+model_version_prefix+\".parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cbe07-8f8b-46ce-be8a-ca048b850ec4",
   "metadata": {},
   "source": [
    "##  Credit cards transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027baf39-86ba-4e9f-88f4-2bed669e03c2",
   "metadata": {},
   "source": [
    "##  Customer and Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bce7d-eee4-4e5b-852b-0c8dbe35a23c",
   "metadata": {},
   "source": [
    "## Bank Online Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b7423-4578-488b-b734-9dd5fa98e1b1",
   "metadata": {},
   "source": [
    "## Loan History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45981e6-7dad-4c20-bed2-cc169a9df2c8",
   "metadata": {},
   "source": [
    "## Customer 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00663f2-4939-47cd-8745-43f7089cd974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark_t1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
